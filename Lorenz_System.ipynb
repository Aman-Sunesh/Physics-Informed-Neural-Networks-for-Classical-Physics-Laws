{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c602576",
   "metadata": {},
   "source": [
    "# Solving the Lorenz System Using Physics-Informed Neural Networks (PINNs)\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The **Lorenz system** is a classic example of a 3-variable chaotic ODE system originally derived from simplified convection rolls in the atmosphere. It is defined by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dot x &= \\sigma\\,(y - x),\\\\\n",
    "\\dot y &= x\\,(\\rho - z) - y,\\\\\n",
    "\\dot z &= x\\,y - \\beta\\,z,\n",
    "\\end{aligned}\n",
    "\\qquad t \\ge 0\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\(x(t), y(t), z(t)\\) are the state variables.  \n",
    "- \\($\\sigma$\\), \\($\\rho$\\), and \\($\\beta$\\) are system parameters.  \n",
    "- We enforce the **initial condition**  \n",
    "  $$\n",
    "  (x(0),\\,y(0),\\,z(0)) = (x_0,\\,y_0,\\,z_0).\n",
    "  $$\n",
    "\n",
    "Typical parameter values for the canonical “Lorenz attractor” are:  \n",
    "- \\($\\sigma$ = 10\\)  \n",
    "- \\($\\rho$ = 28\\)  \n",
    "- \\($\\beta$ = $\\tfrac{8}{3}$\\)  \n",
    "\n",
    "A common choice of initial state is \\((x_0,y_0,z_0) = (1,1,1)\\).\n",
    "\n",
    "---\n",
    "\n",
    "## Chaotic Dynamics\n",
    "\n",
    "- No closed-form solution exists for arbitrary \\(t\\).  \n",
    "- Exhibits **sensitive dependence on initial conditions** (chaos).  \n",
    "- Trajectories for \\($\\rho$ > 24.74\\) converge to the famous **Lorenz attractor**.\n",
    "\n",
    "---\n",
    "\n",
    "## PINN Objective\n",
    "\n",
    "We approximate each state by a neural network  \n",
    "\\[\n",
    "(\\,\\hat x(t;\\theta),\\;\\hat y(t;\\theta),\\;\\hat z(t;\\theta)\\,)\n",
    "\\]\n",
    "and train to minimize the combined loss\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\n",
    "=\n",
    "\\underbrace{\\mathrm{MSE}_{\\rm res}}_{\\text{ODE residuals}}\n",
    "\\;+\\;\n",
    "\\underbrace{\\mathrm{MSE}_{\\rm ic}}_{\\text{initial conditions}}.\n",
    "$$\n",
    "\n",
    "- **Residual loss** (enforces the Lorenz ODEs at residual points \\(t_i\\)):  \n",
    "  $$\n",
    "  \\mathrm{MSE}_{\\rm res}\n",
    "  = \\frac{1}{N_r} \\sum_{i=1}^{N_r}\n",
    "  \\Bigl[\n",
    "    \\dot{\\hat x}(t_i) - \\sigma\\bigl(\\hat y(t_i)-\\hat x(t_i)\\bigr)\n",
    "  \\Bigr]^2\n",
    "  + \n",
    "  \\Bigl[\n",
    "    \\dot{\\hat y}(t_i) - \\bigl(\\hat x(t_i)\\,(\\rho-\\hat z(t_i)) - \\hat y(t_i)\\bigr)\n",
    "  \\Bigr]^2\n",
    "  + \n",
    "  \\Bigl[\n",
    "    \\dot{\\hat z}(t_i) - \\bigl(\\hat x(t_i)\\,\\hat y(t_i) - \\beta\\,\\hat z(t_i)\\bigr)\n",
    "  \\Bigr]^2.\n",
    "  $$\n",
    "\n",
    "- **Initial condition loss**:  \n",
    "  $$\n",
    "  \\mathrm{MSE}_{\\rm ic}\n",
    "  = (\\hat x(0)-x_0)^2\n",
    "  + (\\hat y(0)-y_0)^2\n",
    "  + (\\hat z(0)-z_0)^2.\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use PINNs?\n",
    "\n",
    "- **Mesh-free, continuous surrogate**: yields smooth \\(\\hat x(t)\\), \\(\\hat y(t)\\), \\(\\hat z(t)\\) for all \\(t\\).  \n",
    "- **Physics-aware loss**: directly enforces the ODE system, even in chaotic regimes.  \n",
    "- **Automatic differentiation**: computes \\(\\dot{\\hat x}\\), \\(\\dot{\\hat y}\\), \\(\\dot{\\hat z}\\) easily.  \n",
    "- **Flexible sampling**: choose residual points anywhere in \\([0,T]\\) to capture rapid divergence.\n",
    "\n",
    "Let’s now define our network architecture, sample residual and IC points, and train the PINN to reproduce the Lorenz attractor trajectories!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df318625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb1b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode(t, x, y, z, sigma, rho, beta):\n",
    "    t = t.clone().detach().requires_grad_(True)\n",
    "    out = network(t)      # [N,3]\n",
    "    x = out[:, 0:1]       \n",
    "    y = out[:, 1:2]\n",
    "    z = out[:, 2:3]\n",
    "\n",
    "    dx_dt = torch.autograd.grad(x, t, grad_outputs=torch.ones_like(x), create_graph=True)[0]\n",
    "    dy_dt = torch.autograd.grad(y, t, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "    dz_dt = torch.autograd.grad(z, t, grad_outputs=torch.ones_like(z), create_graph=True)[0]\n",
    "\n",
    "    f_x = sigma * (y_t - x_t)\n",
    "    f_y = (x * (rho - z)) - y\n",
    "    f_z = (x_t * y_t) - (beta)\n",
    "\n",
    "    res_x = dx_dt - f_x\n",
    "    res_y = dy_dt - f_y\n",
    "    res_z = dz_dt - f_z\n",
    "\n",
    "    return res_x, res_y, res_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff41a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(network, t, x, y, z, t0, x0, y0, z0, sigma, rho, beta):\n",
    "    # ODE residual loss\n",
    "    res_x, res_y, res_z = ode(t, x, y, z, sigma, rho, beta)\n",
    "    MSE_res = torch.mean(res_x**2) + torch.mean(res_y**2) + torch.mean(res_z**2)\n",
    "\n",
    "    # initial-condition loss\n",
    "    T0_pred = network(t0)      # [N,3]\n",
    "    x0_pred = T0_pred[:, 0:1]       \n",
    "    y0_pred = T0_pred[:, 1:2]\n",
    "    z0_pred = T0_pred[:, 2:3]\n",
    "    MSE_ic = torch.mean((x0_pred  - x0)**2) + torch.mean((y0_pred  - y0)**2) + torch.mean((z0_pred  - z0)**2)\n",
    "\n",
    "    return MSE_res + MSE_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, t, x, y, z, t0, x0, y0, z0, sigma, rho, beta, epochs, lr = 1e-3):\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "        loss_value = loss(network, t, x, y, z, t0, x0, y0, z0, sigma, rho, beta)\n",
    "        loss_list.append(loss_value.item())\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch}/{epochs} — Loss: {loss_value.item():.3e}\")\n",
    "\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed4549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
